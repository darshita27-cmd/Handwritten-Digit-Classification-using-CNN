# -*- coding: utf-8 -*-
"""semi_supervised.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jMxG9ODPwySdw5m9Yzy70wyBZeN1hoPv

Handwritten Digit Recognition
"""

!pip install tensorflow numpy matplotlib sklearn

import numpy as np # for numerical data, formulas
import numpy as np # for numerical data, formulas
import matplotlib.pyplot as plt # for better visulation
import tensorflow as tf # tensorflow support many deep learning libraries and machine learning models. tensorflow operates on tensors which are multi-dimentionality array. it aslo has keras which have multiple high level API's
from tensorflow.keras.datasets import mnist # keras is a high level neural network. keras simplifies the process of building and training deep learning models. minst is a dataset for handwritten digits
from tensorflow.keras.models import Sequential # sequesntital is so that we can add linear stack of layers used to create a neural network
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense # Conv2D is a layer of Convolutional neural network(CNN). Conv2D is used for 2D convolutional operations to process image data. MaxPooling2D is also a laer of Convolutional operations which is used for downsampling the data  ( reducing dimenions with keeping the important information ). FLatten is a layer of Convolutional neural network. FLatten converts the multidimentional array into one dimensinal array which is import before feeding it to the dense layers. Dense is a fully connected layer where each neuron is connected to the other layer, used for output layer
from sklearn.semi_supervised import SelfTrainingClassifier # sklearn is a library that provides the tools for data mining and data analysis. SelfTraningClassifier is a semi-supervised learing algorithm.it gives labels to the unlabeled data based on the predictions that it learned from the labeled data
from sklearn.metrics import accuracy_score # accuracy score will calculate how well the model performed
from sklearn.semi_supervised import SelfTrainingClassifier # sklearn is a library that provides the tools for data mining and data analysis. SelfTraningClassifier is a semi-supervised learing algorithm.it gives labels to the unlabeled data based on the predictions that it learned from the labeled data
from sklearn.metrics import accuracy_score # accuracy score will calculate how well the model performed
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dropout
from tensorflow.keras.callbacks import EarlyStopping

#-------------------------------------------step 2 load the dataset ---------------
(x_train_full, y_train_full), (x_test, y_test)=mnist.load_data() # mnist.load_data() is the dataset that contains the 70000 images of the handwritten digits (0 to 9) in two parts train and test. the trainig has x_train_full which is a 3D array with labeled data and has shape of (60000, 28, 28) =(number of training imanges, pixels, pixels). y_train_full on the other hand is a 1D array of labeled data too with shape( 60000). x_test is a 3D array is also with labeled data with shape ( 10000,28,28). y_test is a 1D array, labeled data, shape (10000)
x_train_full=x_train_full/255.0 # here we are normalizing. x_train has pixel coverage from [0,255] and we are converting in [0,1]. pixel with 0 will remain 0 (0/255=0) and example 128/255=0.502. we are normalizing so that we ca improve the coverage for neural networs( neural network covers faster with input features on similar scale). it also helps to svoid saturation ( if we use the sigmoid ir tan functions than they can get saturated over greater values). it also helps in numerical stability so that gradients won't be explode or vanish. gradients are the mathematical concept that represents the change in the function. now, exploding gradient -> its like a snowball. ist thrown from one place to others ( between layers) if this collects too much snow ( as weight in cnn can increase rapidly) than it will lose track and model won't be able to learn properly. vanishing gradient-> if the values becomes too less almost equal to zero
x_test=x_test/255.0
# we didn't divide the y values with 255 as theya re not numerical but the categorial data

# now, we will select only the small labeled images
num_labeled=1000 # will be taking 100 labeled images
x_labeled=x_train_full[:num_labeled] # slicing the first 100 images of the training set
y_labeled=y_train_full[:num_labeled]


# other than 100 images we will be using unlabeld data
x_unlabeled=x_train_full[num_labeled:]
y_unlabeled=np.full(len(x_unlabeled), -1) # len(x_unlabeled) gives the rest of the images which are left 59900. np.full(len(unlabeled),-1) creates the array of the same size as x_unlabeled filled with -1. -1 means that these images don't have labels. though we can use any values instead of -1 just make sure they don't overlap with other values exaple we have 0-9 for numerical, 0 to 1 normalized pixels avoid using these. can use -999 pr anything as long as they don't overlap

x_labeled= tf.image.resize(x_labeled[...,np.newaxis],[20,20]).numpy().squeeze() # converting the 28 X 28 size image into 14 X 14 for so that ram will be less used and later inputs for standardization will be lower. we convert the it into numpy array that would give (14, 14, 1) here 1 is the dimention which will be removed by squeeze because we won't need it later. the '...' epsilons here are used to select all the dimensions except the ones used earlier ( used when have multiple dimensions)
x_unlabeled=tf.image.resize(x_unlabeled[...,np.newaxis],[20,20]).numpy().squeeze()
x_test=tf.image.resize(x_test[...,np.newaxis], [20,20]).numpy().squeeze()

x_labeled,x_val,y_labeled,y_val=train_test_split(x_labeled,y_labeled,test_size=0.2,random_state=42)

# simple convolutional neural network because we are using limited labeled data
def create_model():
  model=Sequential([ # sequential model allows to stack linearly where each layer has exactly one input tensor and one output tensor
      Conv2D(32,(3,3), activation='relu', input_shape=(20,20,1)), # Conv2D is a 2D convolutional layer that. Conv2D applys filters( or kernals). here 32 kernals or filters are used. each filter learns will learn to detect different features in input image (like edges, texture, etc). (3,3) is the size of each filter or say 3 pixels wide and 3 pixels tall windowsize filter will be applied to the input images. activation=relu relu(Rectified linear unit) helps find the non-linearity. if the input is positive than it gives the output else it gives the output as 0 (explamle if input is 3 than the output will be 3 and if the input is -2 than output will be 0) this helps the model to learn complex patterns. input_shape=(14,14,1) it means that the input images are 14 by 14 pixels in size and 1 represents that they are grayscale
      MaxPooling2D((2,2)),
      Dropout(0.25),
      Conv2D(64,(3,3),activation='relu'),
      MaxPooling2D((2,2)), # maxpooling gives a lower window frame. for example if the input is 4X4 and we are giving the size of 2X2 than it will get the most import features. maxpooling also helps in reducing the overfitting by reducing the number of parameter intake from input imgaes
      Dropout(0.25), # Dropout helps to prevent the overfitting. it randomly dropout a fraction of neural network setting it to zero. when we gave dropout 0.25 than it means that each neuron has a probability of 0.25 of being dropped. as we are droping the neurons randomly,it prevents model to rely on a particular neuron heavily.
      Flatten(), # this layer flattens the 2D features to 1D vector. it is nessary to do before sending the data to dense layer
      Dense(128, activation='relu'), # dense layer here has 32 neurons and relu as activation function. relu means that if the input is positive tha it displays the input as output and if its negative than it shows the output as 0 example:input  x=3 than output=3 and if input is -2 than output is 0
      Dropout(0.5),
      Dense(10, activation='softmax') # dense is a layer in keras that creates a fully connected layer. each neuron is connected to every neuron in the previous layer. we have numbers as output from 0 to 9 ( mnist dataset ) therefore we need 10 neurons in the output layer to represent each class. softmax converts the raw output from the neurons to the probabilities. softmax: here we first do the exponantional and than normalize by dividing each exponantional with the total normalization and its gives us the output between 0 and 1 and the sum of these output is 1.example i got a vector from the output layer as [2.0,1.0,0.1]. -> then we do the exponantial, [e^(2.0),e^(1.0),e^(0.1)] and got the values [7.39,2.77,1.11]->we normalize to do that sum=7.39+2.72+1.11 (=11.22) -> divide each exponantial value with e^(2.0) / 11.22 similarly with each exponential. we get [0.66,0.24,0.10] and if we add [0.66+0.24+0.10]=1. it means class 1 66% class 2 24% class 3 10%
  ])
  model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy']) # optimizer helps to find the route to the best prediction. adam can adapatively changes giw slowly or quickly it learns which leads to faster and stabel learning.
  return model

#-------------------training the model on small dataset------------------
early_stopping=EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)
model=create_model()
history=model.fit(x_labeled,y_labeled,epochs=20,batch_size=32,validation_data=(x_test,y_test),callbacks=[early_stopping]) # epcho is like how many times the model will go through the data.batch size is the number of samples taken before updating internal parameters. validation data helps to find the performance after each epoch. early stopping helps to prevent overfitting by stopping the training process when the validation set stops improving

#-----------------self training: using the model to label unlabeled data------------
# predicting probabilities for unlabeled data
predictions=model.predict(x_unlabeled)
confidence_score=np.max(predictions,axis=1) # confidence score means how certain the modele is that a particular sample belongs to a particular class. if we give a sample to the model than it will see how it matches the other numbers and will get the one with hightest probability
pseudo_labels=np.argmax(predictions,axis=1) # argmax helps find the maximum value and d there are many it will return the first one

# selecting only the highly confidence predictions
high_confidence=confidence_score>0.80 # threshold
x_pseudo=x_unlabeled[high_confidence]
y_pseudo=pseudo_labels[high_confidence]

#-------------------adding the pseudo labeled data and retrain the data ----------------
x_combined=np.concatenate([x_labeled,x_pseudo])
y_combined=np.concatenate([y_labeled,y_pseudo])

model.fit(x_combined,y_combined, epochs=10,batch_size=32,validation_data=(x_test,y_test))

#---------------evaluating the final model on test data---------------------
test_loss,test_acc=model.evaluate(x_test,y_test)
print(f"final model accuracy on test data: {test_acc:.2f}")

# visualizing predictions on test images
def plot_predictions(images, true_labels, predictions, num=10):
  plt.figure(figsize=(15,5))
  for i in range(num):
    plt.subplot(2,5,i+1)
    plt.imshow(images[i],cmap='gray')
    plt.title(f"True: {true_labels[i]} \n predicted: {predictions[i]}")
    plt.axis("off")
  plt.tight_layout()
  plt.show()

test_predictions=model.predict(x_test)
predicted_labels=np.argmax(test_predictions,axis=1)
plot_predictions(x_test,y_test,predicted_labels,num=10)